# Proposal: Hephaestus v0.1.1 Implementation

## 1. Overview

This proposal outlines the implementation plan for Hephaestus v0.1.1, which focuses on transforming the current architectural foundation into a fully functional self-improving system. The primary objective is to bridge the gap between the structured architecture and actual code generation capabilities by implementing LLM integration, improving the FlowBuilder functionality, and establishing a basic testing framework.

## 2. Current State Assessment

The current v0.1.0 implementation has established a solid foundation with:
- Core Node/Flow infrastructure following the PocketFlow pattern
- Essential components (GoalProposer, MutationEngine, TestHarness, Scoring, Registry)
- Basic orchestration via ForgeLoop
- Configuration through environment variables and CLI arguments

However, key limitations prevent the system from being fully functional:
- FlowBuilder uses placeholder code generation instead of actual LLM integration
- No file saving capabilities in the FlowBuilder
- Lack of testing infrastructure to validate system behavior
- Limited registry querying and visualization

## 3. Implementation Objectives

The v0.1.1 implementation will focus on three key areas:

### 3.1 LLM Service Integration
- Create a unified LLM service interface for different providers
- Implement integrations for OpenAI and Anthropic APIs
- Develop specialized prompt templates for code generation tasks
- Add context management for improved code generation

### 3.2 FlowBuilder Enhancements
- Implement file saving capabilities for generated code
- Add intelligent file naming and path management
- Create import extraction and dependency tracking
- Improve code quality validation on generation

### 3.3 Testing Framework
- Establish unit tests for core components
- Develop integration tests for the complete pipeline
- Create test fixtures and mock LLM responses
- Implement automated test runners for CI/CD

## 4. Technical Implementation Details

### 4.1 LLM Service Module

```
services/
├── llm_service.py      # Base LLM service abstract class
├── openai_service.py   # OpenAI API integration
├── anthropic_service.py # Anthropic API integration
└── prompt_templates/   # Specialized prompt templates
    ├── node_creation.py
    ├── flow_creation.py
    └── code_review.py
```

The `llm_service.py` will define a common interface:

```python
class LLMService(ABC):
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generate text from a prompt"""
        pass
        
    @abstractmethod
    async def generate_with_template(self, template_name: str, **kwargs) -> str:
        """Generate text using a predefined template"""
        pass
```

### 4.2 FlowBuilder Enhancements

Extend the existing FlowBuilderNode with:

```python
def post(self, shared: Dict[str, Any], prep_res: Dict[str, Any], exec_res: str) -> str:
    """
    Process generated code and save to appropriate location.
    
    Args:
        shared: The shared data store
        prep_res: Preparation results
        exec_res: Generated code
        
    Returns:
        Action string for flow control
    """
    # Extract class/function names from generated code
    code_metadata = self._extract_code_metadata(exec_res)
    
    # Determine file path based on metadata
    file_path = self._determine_file_path(code_metadata)
    
    # Save the file
    self._save_code_to_file(file_path, exec_res)
    
    # Update shared store
    shared["code"] = exec_res
    shared["code_metadata"] = code_metadata
    shared["file_path"] = file_path
    
    return "default"
```

### 4.3 Testing Framework

Create a `tests` directory with pytest-based tests:

```
tests/
├── unit/
│   ├── test_node.py
│   ├── test_flow.py
│   ├── test_registry.py
│   └── test_scoring.py
├── integration/
│   ├── test_forge_loop.py
│   └── test_end_to_end.py
└── fixtures/
    ├── mock_llm_responses.py
    └── sample_code_outputs.py
```

## 5. Implementation Timeline

| Week | Focus Area | Tasks |
|------|------------|-------|
| 1 | LLM Service - Foundation | Create base service and OpenAI integration |
| 1 | Testing - Setup | Establish testing framework and initial unit tests |
| 2 | LLM Service - Templates | Develop prompt templates for different tasks |
| 2 | FlowBuilder - File Management | Implement file saving and path management |
| 3 | FlowBuilder - Metadata | Add code metadata extraction and imports parsing |
| 3 | LLM Service - Anthropic | Add Anthropic API integration |
| 4 | Testing - Integration | Create integration tests for the full pipeline |
| 4 | Final Integration | Connect all components and verify operation |

## 6. Success Criteria

The v0.1.1 implementation will be considered successful when:

1. **LLM Integration**
   - The system can generate syntactically valid Python code using actual LLM calls
   - Both OpenAI and Anthropic APIs are supported
   - Specialized prompt templates improve code quality

2. **FlowBuilder Functionality**
   - Generated code is correctly saved to appropriate file locations
   - Code imports and dependencies are properly tracked
   - File naming follows a consistent pattern

3. **Testing Framework**
   - 80%+ test coverage for core components
   - Integration tests verify the complete pipeline operation
   - CI/CD process is established

4. **Overall System Function**
   - The system can complete at least one full forge loop cycle autonomously
   - Generated code passes syntax validation and basic quality checks
   - The registry correctly tracks builds and their relationships

## 7. Future Considerations

While implementing v0.1.1, we should lay groundwork for future enhancements:

- **Registry Visualization**: Design storage format to support future visualization
- **Specialized Node Types**: Structure the codebase to allow easy addition of specialized nodes
- **Performance Metrics**: Add instrumentation for gathering performance data

## 8. Conclusion

The v0.1.1 implementation represents a critical step in Hephaestus's evolution from a structural framework to a functional self-improving system. By focusing on LLM integration, file handling, and testing, we can create a solid foundation for more advanced features while ensuring the current system works as intended. 